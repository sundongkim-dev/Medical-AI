{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /disk1/colonoscopy_dataset/cropped/ADC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Data는 총 4 가지가 있다. \n",
    "- ADC (ADeno-Carcinoma)\n",
    "- HGD (High-Grade Dysplasia)\n",
    "- LGD (Low-Grade Dysplasia)\n",
    "- NOR (Normal)\n",
    "\n",
    "데이터의 생김새는 다음과 같다. Raw image와 masking이 된(labeled) image가 있다.\n",
    "- data: \\[phase\\]\\_IMG_[patient #].jpg\n",
    "- label: \\[phase\\]\\_MASK_[patient #].jpg\n",
    "\n",
    "데이터들은 original data에서 검은 부분을 최대한 지운, 즉 cropped 된 상태이다. \n",
    "이미지의 사이즈는 다 달라서 resize를 해주어야 한다.\n",
    "\n",
    "가) ADC의 데이터는 patient의 image와 mask image로 각각 data와 label로 나누어 이해하면 된다. data의 수와 그에 따라 label이 있는지 확인해보자.\n",
    "\n",
    "나) 3개씩 뽑아서 어떤 형태인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualize를 위한 imports\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "# GPU 한 개만 할당을 위함\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "pd.set_option('display.max.colwidth', 30)\n",
    "\n",
    "# 각 이미지들이 속해 있는 경로\n",
    "ADC_img_path = '/disk1/colonoscopy_dataset/cropped/ADC/'\n",
    "HGD_img_path = '/disk1/colonoscopy_dataset/cropped/HGD/'\n",
    "LGD_img_path = '/disk1/colonoscopy_dataset/cropped/LGD/'\n",
    "NOR_img_path = '/disk1/colonoscopy_dataset/cropped/NOR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가) 전체 파일 수, ADC data 수와 labeled data 수 확인\n",
    "\n",
    "ADC_file_list = os.listdir(ADC_img_path) # ADC 전체 파일 목록\n",
    "HGD_file_list = os.listdir(HGD_img_path) # HGD 전체 파일 목록\n",
    "LGD_file_list = os.listdir(LGD_img_path) # LGD 전체 파일 목록\n",
    "NOR_file_list = os.listdir(NOR_img_path) # NOR 전체 파일 목록\n",
    "\n",
    "ADC_data_list = [x for x in ADC_file_list if 'IMG' in x]        # ADC data 파일 목록\n",
    "ADC_labeled_list = [x for x in ADC_file_list if 'MASK' in x]    # ADC mask 파일 목록\n",
    "\n",
    "HGD_data_list = [x for x in HGD_file_list if 'IMG' in x]        # HGD data 파일 목록\n",
    "HGD_labeled_list = [x for x in HGD_file_list if 'MASK' in x]    # HGD mask 파일 목록\n",
    "\n",
    "LGD_data_list = [x for x in LGD_file_list if 'IMG' in x]        # LGD data 파일 목록\n",
    "LGD_labeled_list = [x for x in LGD_file_list if 'MASK' in x]    # LGD mask 파일 목록\n",
    "\n",
    "NOR_data_list = [x for x in NOR_file_list if 'IMG' in x]        # NOR data 파일 목록\n",
    "NOR_labeled_list = [x for x in NOR_file_list if 'MASK' in x]    # NOR mask 파일 목록\n",
    "\n",
    "totalNum_list = [len(ADC_file_list), len(HGD_file_list), len(LGD_file_list), len(NOR_file_list)]\n",
    "totalData_list = [len(ADC_data_list), len(HGD_data_list), len(LGD_data_list), len(NOR_data_list)]\n",
    "totalMask_list = [len(ADC_labeled_list), len(HGD_labeled_list), len(LGD_labeled_list), len(NOR_labeled_list)]\n",
    "\n",
    "data = [totalNum_list, totalData_list, totalMask_list]\n",
    "table = pd.DataFrame(data = data, index = ['Total #', 'data #', 'mask data #'], columns = ['ADC', 'HGD', 'LGD', 'NOR'])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나) 3개씩 뽑아서 확인해보자.\n",
    "print(\"[1] ADC [2] HGD [3] LGD [4] NOR\") # 각 번호에 맞는 데이터를 보여준다.\n",
    "#num = input()\n",
    "num = '1'\n",
    "\n",
    "path_dic = {\"1\": ADC_img_path, \"2\": HGD_img_path, \"3\": LGD_img_path, \"4\": NOR_img_path}\n",
    "file_dic = {\"1\": ADC_data_list, \"2\": HGD_data_list, \"3\": LGD_data_list, \"4\": NOR_data_list}\n",
    "title_dic = {\"1\": \"ADC\", \"2\": \"HGD\", \"3\": \"LGD\", \"4\": \"NOR\"}\n",
    "\n",
    "rows = 3\n",
    "columns = 2\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "random_file_list = (np.random.choice(file_dic[num], 3)).tolist()\n",
    "match_file_list = [x.replace('IMG', 'MASK') for x in random_file_list]\n",
    "# np.array(Image.open(os.path.join(self.data_dir, label_path)))\n",
    "cnt = 0\n",
    "for a, b in zip(random_file_list, match_file_list):\n",
    "    # IMG\n",
    "    cnt += 1\n",
    "    img = Image.open(os.path.join(path_dic[num], a))\n",
    "    #print(np.array(Image.open(os.path.join(path_dic[num], a)).resize((572,572))).min())\n",
    "    ti = title_dic[num] + '_IMG_' + a[-9:-4]\n",
    "    plt.subplot(rows, columns, cnt)\n",
    "    plt.title(ti)\n",
    "    plt.imshow(img)\n",
    "    cnt += 1\n",
    "    # MASK\n",
    "    img = Image.open(os.path.join(path_dic[num], b))\n",
    "    ti = title_dic[num] + '_IMG_' + b[-9:-4]\n",
    "    #print(np.array(Image.open(os.path.join(path_dic[num], b)).resize((572,572))).shape)\n",
    "    plt.subplot(rows, columns, cnt)\n",
    "    plt.title(ti)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (train, valid) 준비\n",
    "\n",
    "우선적으로 ADC에 대해서만 학습할 계획이다.\n",
    "\n",
    "가) 갖고 있는 504장을 8:2으로 나누어서 dataloader를 구현하자. (train:valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 구현하기\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None, type=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.type = type\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_label = [f for f in lst_data if 'MASK' in f]\n",
    "        lst_input = [f for f in lst_data if 'IMG' in f]\n",
    "\n",
    "        lst_label.sort()\n",
    "        lst_input.sort()\n",
    "\n",
    "        target_label = []\n",
    "        target_input = []\n",
    "\n",
    "        train_length = int(len(lst_input)*0.8)\n",
    "        # train mode\n",
    "        if type == 0:\n",
    "            target_label = lst_label[:train_length]\n",
    "            target_input = lst_input[:train_length]\n",
    "        # test mode\n",
    "        else:\n",
    "            target_label = lst_label[train_length:]\n",
    "            target_input = lst_input[train_length:]\n",
    "        \n",
    "        self.lst_label = target_label\n",
    "        self.lst_input = target_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.asarray(((Image.open(os.path.join(self.data_dir, self.lst_label[index]))).resize((256,256))))\n",
    "        input = np.asarray((Image.open(os.path.join(self.data_dir, self.lst_input[index])).resize((256,256))))\n",
    "\n",
    "        # 정규화\n",
    "        label[label>150] = 255\n",
    "        label[label<=150] = 0\n",
    "        \n",
    "        label = label/255.0\n",
    "        input = input/255.0\n",
    "\n",
    "        # 이미지와 레이블의 차원 = 2일 경우(채널이 없을 경우, 흑백 이미지), 새로운 채널(축) 생성\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        # transform이 정의되어 있다면 transform을 거친 데이터를 불러옴\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
    "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "\n",
    "class Normalization(object):\n",
    "    def __init__(self, mean=0.5, std=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        input = (input - self.mean) / self.std\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data\n",
    "\n",
    "class RandomFlip(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = np.flipud(label)\n",
    "            input = np.flipud(input)\n",
    "\n",
    "        data = {'label': label, 'input': input}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 네트워크 저장하기\n",
    "def save(ckpt_dir, model, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'model': model.state_dict(), 'optim': optim.state_dict()},\n",
    "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
    "\n",
    "## 네트워크 불러오기\n",
    "def load(ckpt_dir, model, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return model, optim, epoch\n",
    "\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    model.load_state_dict(dict_model['model'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return model, optim, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 파라미터 설정하기\n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "num_epoch = 70\n",
    "\n",
    "original_data_path = '/disk1/colonoscopy_dataset/cropped/LGD'\n",
    "\n",
    "base_dir = '/home/sundongk/Resnet18_Unet'\n",
    "data_dir = original_data_path\n",
    "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
    "log_dir = os.path.join(base_dir, \"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #Normalization(mean=0.5, std=0.5), RandomFlip(), \n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train = Dataset(data_dir=original_data_path, transform=transform, type=0)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "dataset_val = Dataset(data_dir=original_data_path, transform=transform, type=1)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': loader_train,\n",
    "    'val': loader_val\n",
    "}\n",
    "\n",
    "print(loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.models\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "  return nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "    nn.ReLU(inplace=True),\n",
    "  )\n",
    "\n",
    "## 네트워크 구축하기\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "checkpoint_path = os.path.join(base_dir, 'chk')\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, st_epoch, num_epochs=25):\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(st_epoch+1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for batch, data in enumerate(dataloaders[phase]):\n",
    "                labels = data['label'].to(device)\n",
    "                inputs = data['input'].to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "            \n",
    "            # save the model weights\n",
    "            if phase == 'val':\n",
    "                val_loss.append(epoch_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(f\"saving best model to {ckpt_dir}\")\n",
    "                    best_loss = epoch_loss\n",
    "                    save(ckpt_dir=checkpoint_path, model=model, optim=optimizer, epoch=epoch)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model, optim, st_epoch = load(ckpt_dir=checkpoint_path, model=model, optim=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "\n",
    "st_epoch = 0\n",
    "num_class = 1\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "# for l in model.base_layers:\n",
    "#   for param in l.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, st_epoch, num_epochs=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 시각화\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(train_loss, 'b', label='train loss')\n",
    "plt.plot(val_loss, 'g', label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sundongk/Resnet18_Unet\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(base_dir)\n",
    "checkpoint_path = os.path.join(base_dir, 'chk')\n",
    "model, optimizer_ft, st_epoch = load(ckpt_dir=checkpoint_path, model=model, optim=optimizer_ft)\n",
    "print(st_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataset_train = Dataset(data_dir=original_data_path, transform=transform, type=0)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "dataset_val = Dataset(data_dir=original_data_path, transform=transform, type=1)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "def changeTensor(tensor, labelmode=0, threshold=0.1):\n",
    "    image = tensor.detach().cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    if labelmode == 1:\n",
    "        image[image>threshold] = 1\n",
    "        image[image<=threshold] = 0 \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel Accuracy / Intersection Over Union / Dice coefficient(=F1 score)\n",
    "def get_PA_IOU_and_DICE(pred, true, TH):\n",
    "    pred = changeTensor(pred, 1, TH)\n",
    "    true = changeTensor(true, 1, TH)\n",
    "    \n",
    "    pred = pred.detach().cpu().numpy().reshape(-1)\n",
    "    true = true.detach().cpu().numpy().reshape(-1)\n",
    "    \n",
    "    size = len(pred)^2\n",
    "    pa = (size - np.logical_xor(true, pred).sum()) / size\n",
    "    \n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    \n",
    "    if np.sum(union) == 0:\n",
    "        iou = -1\n",
    "    else:\n",
    "        iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    intersection = (pred * true).sum()\n",
    "    dice = (2.*intersection + 1) / (pred.sum() + true.sum() + 1)\n",
    "\n",
    "    return pa, iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n",
      "0.9248\n",
      "0.7502\n",
      "0.848\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "TH = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "# Results(various metrics) for Training set\n",
    "for th in TH:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc_PA = []\n",
    "        acc_IOU = []\n",
    "        acc_DICE = []\n",
    "        \n",
    "        for batch, data in enumerate(loader_val, 1):\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            output = model(input)\n",
    "        \n",
    "            # Calculate PA & IOU & Dice coefficient(=F1 score)\n",
    "            pa, iou, dice = get_PA_IOU_and_DICE(output, label, th)\n",
    "            acc_PA.append(pa)\n",
    "            acc_IOU.append(iou)\n",
    "            acc_DICE.append(dice)\n",
    "\n",
    "            # # 불러온 이미지 시각화\n",
    "            # input_image_example = plt.subplot(1,3,1)\n",
    "            # input_image_example.set_title('input Image Example')\n",
    "            # plt.imshow(to_pil_image(changeTensor(input)))\n",
    "\n",
    "            # label_image_example = plt.subplot(1,3,2)\n",
    "            # label_image_example.set_title('Label Image Example')\n",
    "            # plt.imshow(to_pil_image(changeTensor(label)), cmap='gray')\n",
    "\n",
    "            # test_image_example = plt.subplot(1,3,3)\n",
    "            # test_image_example.set_title('Test Image Example')\n",
    "            # plt.imshow(to_pil_image(changeTensor(output, 1)), cmap='gray')\n",
    "\n",
    "            # plt.show()\n",
    "        print(np.round(np.mean(acc_PA),4))\n",
    "        print(np.round(np.mean(acc_IOU),4))\n",
    "        print(np.round(np.mean(acc_DICE), 4))\n",
    "        print(\"-\"*10)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 파라미터 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a28fd041cca8e88761e305ad12b9793255fd9d220f898ef66faf7da0cb355b51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
