{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualize를 위한 imports\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "# GPU 한 개만 할당을 위함\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from configparser import Interpolation\n",
    "from saveLoad import save, load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from util import TwoCropTransform, AverageMeter\n",
    "from util import adjust_learning_rate, warmup_learning_rate\n",
    "from util import set_optimizer, save_model\n",
    "from networks.resnet_big import SupConResNet\n",
    "from losses import SupConLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "pd.set_option('display.max.colwidth', 30)\n",
    "\n",
    "# 경로 설정하기\n",
    "original_data_path = '/disk1/colonoscopy_dataset/cropped/' # ADC / HGD / LGD / NOR\n",
    "base_dir = '/home/sundongk/Multiclass_classification_contrastive'\n",
    "data_dir = original_data_path\n",
    "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
    "root_dir = '/home/sundongk/dataset/'\n",
    "# 각 이미지들이 속해 있는 경로\n",
    "ADC_img_path = '/disk1/colonoscopy_dataset/cropped/ADC/'\n",
    "HGD_img_path = '/disk1/colonoscopy_dataset/cropped/HGD/'\n",
    "LGD_img_path = '/disk1/colonoscopy_dataset/cropped/LGD/'\n",
    "NOR_img_path = '/disk1/colonoscopy_dataset/cropped/NOR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가) 전체 파일 수, ADC data 수와 labeled data 수 확인\n",
    "\n",
    "ADC_file_list = os.listdir(ADC_img_path) # ADC 전체 파일 목록\n",
    "HGD_file_list = os.listdir(HGD_img_path) # HGD 전체 파일 목록\n",
    "LGD_file_list = os.listdir(LGD_img_path) # LGD 전체 파일 목록\n",
    "NOR_file_list = os.listdir(NOR_img_path) # NOR 전체 파일 목록\n",
    "\n",
    "ADC_data_list = sorted([x for x in ADC_file_list if 'IMG' in x])        # ADC data 파일 목록\n",
    "ADC_labeled_list = sorted([x for x in ADC_file_list if 'MASK' in x])   # ADC mask 파일 목록\n",
    "\n",
    "HGD_data_list = sorted([x for x in HGD_file_list if 'IMG' in x])        # HGD data 파일 목록\n",
    "HGD_labeled_list = sorted([x for x in HGD_file_list if 'MASK' in x])    # HGD mask 파일 목록\n",
    "\n",
    "LGD_data_list = sorted([x for x in LGD_file_list if 'IMG' in x])        # LGD data 파일 목록\n",
    "LGD_labeled_list = sorted([x for x in LGD_file_list if 'MASK' in x])    # LGD mask 파일 목록\n",
    "\n",
    "NOR_data_list = sorted([x for x in NOR_file_list if 'IMG' in x])        # NOR data 파일 목록\n",
    "NOR_labeled_list = sorted([x for x in NOR_file_list if 'MASK' in x])    # NOR mask 파일 목록\n",
    "\n",
    "totalNum_list = [len(ADC_file_list), len(HGD_file_list), len(LGD_file_list), len(NOR_file_list)]\n",
    "totalData_list = [len(ADC_data_list), len(HGD_data_list), len(LGD_data_list), len(NOR_data_list)]\n",
    "totalMask_list = [len(ADC_labeled_list), len(HGD_labeled_list), len(LGD_labeled_list), len(NOR_labeled_list)]\n",
    "\n",
    "data = [totalData_list, totalMask_list, totalNum_list]\n",
    "table = pd.DataFrame(data = data, index = ['data #', 'mask data #', 'Total #'], columns = ['ADC', 'HGD', 'LGD', 'NOR'])\n",
    "\n",
    "total_img_name_list = [ADC_data_list, HGD_data_list, LGD_data_list, NOR_data_list]\n",
    "total_label_name_list = [ADC_labeled_list, HGD_labeled_list, LGD_labeled_list, NOR_labeled_list]\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 만들기\n",
    "\n",
    "- Train, validation, test를 각각 8 : 1 : 1의 비율로 나누어 만든다. \n",
    "  - 총 데이터: 3004장 = (2403, 300, 301)\n",
    "  - Train, validation의 batch size는 32로 하고, test의 batch size는 1로 한다.\n",
    "- 현재 task는 <font color=yellow>classification이므로 label을 바이너리</font>로 만들어주어야 한다.\n",
    "  - ADC: 0\n",
    "  - HGD: 1\n",
    "  - LGD: 2\n",
    "  - NOR: 3\n",
    "- 전처리 사항\n",
    "  - 256*256으로 resize + Bilinear interpolation\n",
    "  - random affine (shear=10, scale=(0.8, 1.2))\n",
    "  - random horizontal flip()\n",
    "  - 전체 데이터의 평균과 표준편차로 normalize (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 : 1 : 1\n",
    "ADC_train_length = int(len(ADC_data_list)*0.8) # 403\n",
    "HGD_train_length = int(len(HGD_data_list)*0.8) # 400\n",
    "LGD_train_length = int(len(LGD_data_list)*0.8) # 800\n",
    "NOR_train_length = int(len(NOR_data_list)*0.8) # 800 ----- total 2403\n",
    "\n",
    "ADC_valid_length = int(len(ADC_data_list)*0.1) # 50\n",
    "HGD_valid_length = int(len(HGD_data_list)*0.1) # 50\n",
    "LGD_valid_length = int(len(LGD_data_list)*0.1) # 100\n",
    "NOR_valid_length = int(len(NOR_data_list)*0.1) # 100 ----- total 300\n",
    "\n",
    "ADC_test_length = len(ADC_data_list) - ADC_train_length - ADC_valid_length # 51\n",
    "HGD_test_length = len(HGD_data_list) - HGD_train_length - HGD_valid_length # 50\n",
    "LGD_test_length = len(LGD_data_list) - LGD_train_length - LGD_valid_length # 100\n",
    "NOR_test_length = len(NOR_data_list) - NOR_train_length - NOR_valid_length # 100 ----- total 301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataloader 구현\n",
    "# class Dataset(Dataset):\n",
    "#     def __init__(self, data_dir, transform=None, type=None):\n",
    "#         self.data_dir = data_dir; self.transform = transform; self.type = type\n",
    "\n",
    "#         lst_label = []; lst_input = []; target_input = []; target_label = []\n",
    "\n",
    "#         # if type=0, train mode\n",
    "#         if type == 0:\n",
    "#             target_input = (ADC_data_list[:ADC_train_length] + HGD_data_list[:HGD_train_length] \n",
    "#                                 + LGD_data_list[:LGD_train_length] + NOR_data_list[:NOR_train_length])\n",
    "#             target_label = [0]*ADC_train_length + [1]*HGD_train_length + [2]*LGD_train_length + [3]*NOR_train_length\n",
    "        \n",
    "#         # if type=1, valid mode\n",
    "#         elif type == 1:\n",
    "#             target_input = (ADC_data_list[ADC_train_length : ADC_train_length + ADC_valid_length] \n",
    "#                                 + HGD_data_list[HGD_train_length : HGD_train_length + HGD_valid_length] \n",
    "#                                 + LGD_data_list[LGD_train_length : LGD_train_length + LGD_valid_length] \n",
    "#                                 + NOR_data_list[NOR_train_length : NOR_train_length + NOR_valid_length])\n",
    "#             target_label = [0]*ADC_valid_length + [1]*HGD_valid_length + [2]*LGD_valid_length + [3]*NOR_valid_length\n",
    "\n",
    "#         # if type=2, test mode\n",
    "#         else:\n",
    "#             target_input = (ADC_data_list[ADC_train_length+ADC_valid_length:] \n",
    "#                                 + HGD_data_list[HGD_train_length + HGD_valid_length:] \n",
    "#                                 + LGD_data_list[LGD_train_length + LGD_valid_length:] \n",
    "#                                 + NOR_data_list[NOR_train_length + NOR_valid_length:])\n",
    "#             target_label = [0]*ADC_test_length + [1]*HGD_test_length + [2]*LGD_test_length + [3]*NOR_test_length\n",
    "\n",
    "#         self.lst_label = target_label\n",
    "#         self.lst_input = target_input\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.lst_label)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         datapath = ''\n",
    "#         if 'ADC' in self.lst_input[index]: # 0\n",
    "#             datapath = os.path.join(self.data_dir, 'ADC')\n",
    "#         elif 'HGD' in self.lst_input[index]: # 1\n",
    "#             datapath = os.path.join(self.data_dir, 'HGD')\n",
    "#         elif 'LGD' in self.lst_input[index]: # 2\n",
    "#             datapath = os.path.join(self.data_dir, 'LGD')\n",
    "#         elif 'NOR' in self.lst_input[index]: # 3\n",
    "#             datapath = os.path.join(self.data_dir, 'NOR')\n",
    "        \n",
    "#         input = Image.open(os.path.join(datapath, self.lst_input[index])).convert('RGB')\n",
    "#         if self.transform is not None:\n",
    "#             input = self.transform(input)\n",
    "\n",
    "#         data = {'input': input, 'label': self.lst_label[index]}\n",
    "\n",
    "#         return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋의 평균과 표준편차 구하기\n",
    "# dataset_train = Dataset(data_dir=original_data_path, transform=transforms.ToTensor(), type=0)\n",
    "# full_loader = DataLoader(dataset_train, shuffle=False)\n",
    "\n",
    "# N_CHANNELS = 3\n",
    "# mean = torch.zeros(3)\n",
    "# std = torch.zeros(3)\n",
    "\n",
    "# for item in full_loader:\n",
    "#     inputs = item['input']\n",
    "#     for i in range(N_CHANNELS):\n",
    "#         mean[i] += inputs[:,i,:,:].mean()\n",
    "#         std[i] += inputs[:,i,:,:].std()\n",
    "# mean.div_(len(dataset_train)) # tensor([0.5907, 0.3514, 0.2599]) tensor([0.5875, 0.3497, 0.2598])\n",
    "# std.div_(len(dataset_train)) # tensor([0.2410, 0.1998, 0.1630]) tensor([0.2424, 0.1995, 0.1624])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root_dir, 'train')\n",
    "valid_path = os.path.join(root_dir, 'valid')\n",
    "test_path = os.path.join(root_dir, 'test')\n",
    "\n",
    "batch_size = 32; test_batch_size = 1\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=64, scale=(0.2, 1.)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((100,100)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ]),\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((100,100), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': ImageFolder(root_dir=train_path, transform=TwoCropTransform(data_transforms['train']), target_transform=None),\n",
    "    'validation': ImageFolder(root_dir=valid_path, transform=data_transforms['validation'], target_transform=None),\n",
    "    'test': ImageFolder(root_dir=test_path, transform=data_transforms['test'], target_transform=None),\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=8),\n",
    "    'validation': DataLoader(image_datasets['validation'], batch_size=batch_size, shuffle=False, num_workers=8),\n",
    "    'test': DataLoader(image_datasets['test'], batch_size=test_batch_size, shuffle=False, num_workers=8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50\n",
    "\n",
    "- Supervised contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "\n",
    "# model = models.resnet50(pretrained=True).to(device)\n",
    "\n",
    "# # # freeze\n",
    "# # for param in model.parameters():\n",
    "# #     param.requires_grad = False     \n",
    "\n",
    "# model.fc = nn.Sequential(\n",
    "#                nn.Linear(2048, 256),\n",
    "#                nn.ReLU(inplace=True),\n",
    "#                nn.Linear(256, 4)).to(device)\n",
    "model = SupConResNet()\n",
    "criterion = SupConLoss()\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 loss 함수 정의와 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=0, alpha=None, size_average=True, device='cpu'):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         \"\"\"\n",
    "#         gamma(int) : focusing parameter.\n",
    "#         alpha(list) : alpha-balanced term.\n",
    "#         size_average(bool) : whether to apply reduction to the output.\n",
    "#         \"\"\"\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "#         self.size_average = size_average\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         # input : N * C (btach_size, num_class)\n",
    "#         # target : N (batch_size)\n",
    "\n",
    "#         CE = F.cross_entropy(input, target, reduction='none')  # -log(pt)\n",
    "#         pt = torch.exp(-CE)  # pt\n",
    "#         loss = (1 - pt) ** self.gamma * CE  # -(1-pt)^rlog(pt)\n",
    "\n",
    "#         if self.alpha is not None:\n",
    "#             alpha = torch.tensor(self.alpha, dtype=torch.float).to(self.device)\n",
    "#             # in case that a minority class is not selected when mini-batch sampling\n",
    "#             if len(self.alpha) != len(torch.unique(target)):\n",
    "#                 temp = torch.zeros(len(self.alpha)).to(self.device)\n",
    "#                 temp[torch.unique(target)] = alpha.index_select(0, torch.unique(target))\n",
    "#                 alpha_t = temp.gather(0, target)\n",
    "#                 loss = alpha_t * loss\n",
    "#             else:\n",
    "#                 alpha_t = alpha.gather(0, target)\n",
    "#                 loss = alpha_t * loss\n",
    "\n",
    "#         if self.size_average:\n",
    "#             loss = torch.mean(loss)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "#optimizer = optim.Adam(model.fc.parameters(), lr=0.0001)\n",
    "\n",
    "checkpoint_path_1 = os.path.join(base_dir, 'chk_loss')\n",
    "checkpoint_path_2 = os.path.join(base_dir, 'chk_acc')\n",
    "\n",
    "train_loss = []; val_loss = []; train_acc = []; val_acc = []\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=1, st_epoch=0):\n",
    "    best_loss = 1e10\n",
    "    best_acc = 0\n",
    "    for epoch in range(st_epoch+1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs)); print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train': \n",
    "                model.train()   # Set model to training mode\n",
    "            else:                \n",
    "                model.eval()    # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0; running_corrects = 0\n",
    "\n",
    "            for batch, item in enumerate(dataloaders[phase], 1):\n",
    "                if batch % 10 == 0:\n",
    "                    print('{}/{}'.format(batch, len(dataloaders[phase])))\n",
    "                inputs = item['input'].to(device)\n",
    "                labels = item['label'].to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                #FL = FocalLoss(gamma=2, size_average=True)\n",
    "                #loss = FL(outputs, labels)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                predicts = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                running_corrects += (predicts==labels).sum().item()\n",
    "            \n",
    "            print('{} Accuracy of the model on the {} test images: {}%'.format(phase,\n",
    "                len(image_datasets[phase]), 100 * running_corrects / len(image_datasets[phase])))\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects / len(image_datasets[phase])\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(f\"saving best loss model to {checkpoint_path_1}\")\n",
    "                    best_loss = epoch_loss\n",
    "                    save(ckpt_dir=checkpoint_path_1, model=model, optim=optimizer, epoch=epoch)\n",
    "                if best_acc < epoch_acc:\n",
    "                    print(f\"saving best acc model to {checkpoint_path_2}\")\n",
    "                    best_acc = epoch_acc\n",
    "                    save(ckpt_dir=checkpoint_path_2, model=model, optim=optimizer, epoch=epoch)\n",
    "\n",
    "            time_elapsed = time.time() - since            \n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, st_epoch = load(ckpt_dir=checkpoint_path_1, model=model, optim=optimizer)\n",
    "print(st_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, num_epochs=100, st_epoch=st_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 및 Accuracy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# loss 및 acc 정보 저장\n",
    "loss_dict = {}\n",
    "loss_dict['train_loss'] = train_loss; loss_dict['val_loss'] = val_loss\n",
    "loss_dict['train_acc'] = train_acc; loss_dict['val_acc'] = val_acc\n",
    "\n",
    "# loss_dict 저장\n",
    "with open('data_dict.pkl','wb') as f:\n",
    "    pickle.dump(loss_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# loss_dict 불러오기\n",
    "loss_info_path = '/home/sundongk/Multiclass_classification_with_gradcam/loss/data_dict.pkl'\n",
    "with open(loss_info_path,'rb') as f:\n",
    "    mydict = pickle.load(f)\n",
    "\n",
    "train_loss = mydict['train_loss']; train_acc = mydict['train_acc']\n",
    "val_loss = mydict['val_loss']; val_acc = mydict['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 시각화\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('loss'); plt.xlabel('epoch')\n",
    "plt.plot(train_loss, 'b', label='train loss')\n",
    "plt.plot(val_loss, 'g', label='val loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc 시각화\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('accuracy'); plt.xlabel('epoch')\n",
    "plt.plot(train_acc, 'b', label='train acc')\n",
    "plt.plot(val_acc, 'g', label='val acc')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(); plt.close('all'); plt.clf()\n",
    "model, optimizer, st_epoch = load(ckpt_dir=checkpoint_path_1, model=model, optim=optimizer)\n",
    "print(st_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 데이터셋에 대한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "total = correct = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for batch, item in enumerate(dataloaders['test'], 1):\n",
    "        # if batch == 202:\n",
    "        #     break\n",
    "        inputs = item['input'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        probs = nn.Softmax(dim=1)(outputs)\n",
    "        predicts = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        total += len(labels)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "        # print(probs, predicts, labels)\n",
    "        # # 불러온 이미지 시각화\n",
    "        # plt.figure(figsize=(12,12))\n",
    "        # input_image_example = plt.subplot(1,3,1)\n",
    "        # input_image_example.set_title(f'ADC: {100*pred_probs[0][3]:.2f}, HGD: {100*pred_probs[0][2]:.2f}, LGD: {100*pred_probs[0][1]:.2f}, NOR: {100*pred_probs[0][0]:.2f}')\n",
    "\n",
    "        # plt.imshow(to_pil_image(changeTensor(inputs)))\n",
    "        # print(outputs, labels, pred_probs)\n",
    "        # print(f'NOR: {100*pred_probs[0][0]:.2f}, LGD: {100*pred_probs[0][1]:.2f}, HGD: {100*pred_probs[0][2]:.2f}, ADC: {100*pred_probs[0][3]:.2f}, ')\n",
    "        # plt.show()\n",
    "        # plt.close('all')\n",
    "        # plt.clf()\n",
    "    print('Test Accuracy of the model on the {} test images: {}%'.format(total, 100 * correct / total))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADC: 51, HGD:50, LGD:100, NOR:100\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch, item in enumerate(dataloaders['test'], 1):\n",
    "        inputs = item['input'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(outputs) # save prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # save ground truth\n",
    "\n",
    "class_names = ('ADC', 'HGD', 'LGD', 'NOR')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# Create pandas dataframe\n",
    "dataframe = pd.DataFrame(cf_matrix, index=class_names, columns=class_names)\n",
    "\n",
    "sn.heatmap(dataframe, annot=True, cbar=None,cmap=\"YlGnBu\",fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    " \n",
    "plt.ylabel(\"True Class\"), \n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint as pp\n",
    "\n",
    "pp(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x\n",
    "\n",
    "\n",
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\n",
    "    def __init__(self, model, feature_module, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_module = feature_module\n",
    "        self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations = []\n",
    "        #print(x.shape)\n",
    "        for name, module in self.model._modules.items():\n",
    "            if module == self.feature_module:\n",
    "                target_activations, x = self.feature_extractor(x)\n",
    "            elif \"avgpool\" in name.lower():\n",
    "                x = module(x)\n",
    "                x = x.view(x.size(0),-1)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        \n",
    "        return target_activations, x\n",
    "\n",
    "\n",
    "def preprocess_image(img):\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "    preprocessed_img = img.copy()[:, :, ::-1]\n",
    "    for i in range(3):\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
    "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
    "    preprocessed_img = \\\n",
    "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
    "    preprocessed_img = torch.from_numpy(preprocessed_img)\n",
    "    preprocessed_img.unsqueeze_(0)\n",
    "    input = preprocessed_img.requires_grad_(True)\n",
    "    return input\n",
    "\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    os.path.join(base_dir, \"cam.jpg\")\n",
    "    #cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
    "    cv2.imwrite(os.path.join(base_dir, \"cam.jpg\"), np.uint8(255 * cam))\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, feature_module, target_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.feature_module = feature_module\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "        if self.cuda:\n",
    "            features, output = self.extractor(input.cuda())#[1, 2048, 7, 7], [1, 1000]\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "        #print(index)\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)#1,1000\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "            print(one_hot)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.feature_module.zero_grad()\n",
    "        self.model.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()#1, 2048, 7, 7\n",
    "        target = features[-1]#1,2048,7,7\n",
    "        target = target.cpu().data.numpy()[0, :]#2048, 7, 7\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]#2048\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):#w:weight,target:feature\n",
    "            cam += w * target[i, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)#7,7\n",
    "        cam = cv2.resize(cam, input.shape[2:])#224,224\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADC_img_path = '/disk1/colonoscopy_dataset/cropped/ADC/'\n",
    "# ADC_file_list = os.listdir(ADC_img_path) # ADC 전체 파일 목록\n",
    "# ADC_data_list = sorted([x for x in ADC_file_list if 'IMG' in x])        # ADC data 파일 목록\n",
    "\n",
    "grad_cam = GradCam(model=model, feature_module=model.layer4, target_layer_names=[\"2\"], use_cuda=True)\n",
    "# Inference\n",
    "total = correct = 0\n",
    "model.eval()\n",
    "\n",
    "#with torch.no_grad():\n",
    "running_corrects = 0\n",
    "for item in ADC_data_list:\n",
    "    path = os.path.join(ADC_img_path, item)\n",
    "    img = cv2.imread(path, 1)\n",
    "    img = np.float32(cv2.resize(img, (256, 256))) / 255\n",
    "    input = preprocess_image(img)\n",
    "    \n",
    "    target_index = None\n",
    "    mask = grad_cam(input, target_index)\n",
    "\n",
    "    cam = show_cam_on_image(img, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a28fd041cca8e88761e305ad12b9793255fd9d220f898ef66faf7da0cb355b51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
