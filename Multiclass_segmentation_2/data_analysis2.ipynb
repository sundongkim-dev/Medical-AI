{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 분석\n",
    "Data는 총 4 가지가 있다. \n",
    "- ADC (ADeno-Carcinoma)\n",
    "- HGD (High-Grade Dysplasia)\n",
    "- LGD (Low-Grade Dysplasia)\n",
    "- NOR (Normal)\n",
    "\n",
    "데이터의 생김새는 다음과 같다. Raw image와 masking이 된(labeled) image가 있다.\n",
    "- data: <font color=yellow>\\[phase\\]\\_IMG_[patient #].jpg</font>\n",
    "- label: <font color=yellow>\\[phase\\]\\_MASK_[patient #].jpg</font>\n",
    "\n",
    "데이터들은 original data에서 검은 부분을 최대한 지운, 즉 cropped 된 상태이다. \n",
    "이미지의 사이즈는 다 달라서 resize를 해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualize를 위한 imports\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "# GPU 한 개만 할당을 위함\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from configparser import Interpolation\n",
    "from saveLoad import save, load\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "pd.set_option('display.max.colwidth', 30)\n",
    "\n",
    "# 경로 설정하기\n",
    "original_data_path = '/disk1/colonoscopy_dataset/cropped/' # ADC / HGD / LGD / NOR\n",
    "base_dir = '/home/sundongk/Multiclass_segmentation_2'\n",
    "data_dir = original_data_path\n",
    "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
    "\n",
    "# 각 이미지들이 속해 있는 경로\n",
    "ADC_img_path = '/disk1/colonoscopy_dataset/cropped/ADC/'\n",
    "HGD_img_path = '/disk1/colonoscopy_dataset/cropped/HGD/'\n",
    "LGD_img_path = '/disk1/colonoscopy_dataset/cropped/LGD/'\n",
    "NOR_img_path = '/disk1/colonoscopy_dataset/cropped/NOR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가) 전체 파일 수, ADC data 수와 labeled data 수 확인\n",
    "\n",
    "ADC_file_list = os.listdir(ADC_img_path) # ADC 전체 파일 목록\n",
    "HGD_file_list = os.listdir(HGD_img_path) # HGD 전체 파일 목록\n",
    "LGD_file_list = os.listdir(LGD_img_path) # LGD 전체 파일 목록\n",
    "NOR_file_list = os.listdir(NOR_img_path) # NOR 전체 파일 목록\n",
    "\n",
    "ADC_data_list = sorted([x for x in ADC_file_list if 'IMG' in x])        # ADC data 파일 목록\n",
    "ADC_labeled_list = sorted([x for x in ADC_file_list if 'MASK' in x])   # ADC mask 파일 목록\n",
    "\n",
    "HGD_data_list = sorted([x for x in HGD_file_list if 'IMG' in x])        # HGD data 파일 목록\n",
    "HGD_labeled_list = sorted([x for x in HGD_file_list if 'MASK' in x])    # HGD mask 파일 목록\n",
    "\n",
    "LGD_data_list = sorted([x for x in LGD_file_list if 'IMG' in x])        # LGD data 파일 목록\n",
    "LGD_labeled_list = sorted([x for x in LGD_file_list if 'MASK' in x])    # LGD mask 파일 목록\n",
    "\n",
    "NOR_data_list = sorted([x for x in NOR_file_list if 'IMG' in x])        # NOR data 파일 목록\n",
    "NOR_labeled_list = sorted([x for x in NOR_file_list if 'MASK' in x])    # NOR mask 파일 목록\n",
    "\n",
    "totalNum_list = [len(ADC_file_list), len(HGD_file_list), len(LGD_file_list), len(NOR_file_list)]\n",
    "totalData_list = [len(ADC_data_list), len(HGD_data_list), len(LGD_data_list), len(NOR_data_list)]\n",
    "totalMask_list = [len(ADC_labeled_list), len(HGD_labeled_list), len(LGD_labeled_list), len(NOR_labeled_list)]\n",
    "\n",
    "data = [totalData_list, totalMask_list, totalNum_list]\n",
    "table = pd.DataFrame(data = data, index = ['data #', 'mask data #', 'Total #'], columns = ['ADC', 'HGD', 'LGD', 'NOR'])\n",
    "\n",
    "total_img_name_list = [ADC_data_list, HGD_data_list, LGD_data_list, NOR_data_list]\n",
    "total_label_name_list = [ADC_labeled_list, HGD_labeled_list, LGD_labeled_list, NOR_labeled_list]\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 만들기\n",
    "\n",
    "- Train, validation, test를 각각 8 : 1 : 1의 비율로 나누어 만든다. \n",
    "  - 총 데이터: 3004장 = (2403, 300, 301)\n",
    "  - Train, validation의 batch size는 4로 하고, test의 batch size는 1로 한다.\n",
    "\n",
    "- 전처리 사항\n",
    "  - 256*256으로 resize + Bilinear interpolation\n",
    "  - random affine (shear=10, scale=(0.8, 1.2))\n",
    "  - random horizontal flip()\n",
    "  - 전체 데이터의 평균과 표준편차로 normalize (mean=[0.590, 0.351, 0.259], std=[0.241, 0.199, 0.163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 : 1 : 1\n",
    "ADC_train_length = int(len(ADC_data_list)*0.8) # 403\n",
    "HGD_train_length = int(len(HGD_data_list)*0.8) # 400\n",
    "LGD_train_length = int(len(LGD_data_list)*0.8) # 800\n",
    "NOR_train_length = int(len(NOR_data_list)*0.8) # 800 ----- total 2403\n",
    "\n",
    "ADC_valid_length = int(len(ADC_data_list)*0.1) # 50\n",
    "HGD_valid_length = int(len(HGD_data_list)*0.1) # 50\n",
    "LGD_valid_length = int(len(LGD_data_list)*0.1) # 100\n",
    "NOR_valid_length = int(len(NOR_data_list)*0.1) # 100 ----- total 300\n",
    "\n",
    "ADC_test_length = len(ADC_data_list) - ADC_train_length - ADC_valid_length # 51\n",
    "HGD_test_length = len(HGD_data_list) - HGD_train_length - HGD_valid_length # 50\n",
    "LGD_test_length = len(LGD_data_list) - LGD_train_length - LGD_valid_length # 100\n",
    "NOR_test_length = len(NOR_data_list) - NOR_train_length - NOR_valid_length # 100 ----- total 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 구현\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, type=None):\n",
    "        self.data_dir = data_dir; self.transform = transform; self.type = type\n",
    "\n",
    "        lst_label = []; lst_input = []; target_input = []; target_label = []\n",
    "\n",
    "        # if type=0, train mode\n",
    "        if type == 0:\n",
    "            target_input = (ADC_data_list[:ADC_train_length] + HGD_data_list[:HGD_train_length] \n",
    "                                + LGD_data_list[:LGD_train_length] + NOR_data_list[:NOR_train_length])\n",
    "            target_label = (ADC_labeled_list[:ADC_train_length] + HGD_labeled_list[:HGD_train_length]\n",
    "                                + LGD_labeled_list[:LGD_train_length] + NOR_labeled_list[:NOR_train_length])\n",
    "        \n",
    "        # if type=1, valid mode\n",
    "        elif type == 1:\n",
    "            target_input = (ADC_data_list[ADC_train_length : ADC_train_length + ADC_valid_length] \n",
    "                                + HGD_data_list[HGD_train_length : HGD_train_length + HGD_valid_length] \n",
    "                                + LGD_data_list[LGD_train_length : LGD_train_length + LGD_valid_length] \n",
    "                                + NOR_data_list[NOR_train_length : NOR_train_length + NOR_valid_length])\n",
    "            target_label = (ADC_labeled_list[ADC_train_length : ADC_train_length + ADC_valid_length] \n",
    "                                + HGD_labeled_list[HGD_train_length : HGD_train_length + HGD_valid_length]\n",
    "                                + LGD_labeled_list[LGD_train_length : LGD_train_length + LGD_valid_length] \n",
    "                                + NOR_labeled_list[NOR_train_length : NOR_train_length + NOR_valid_length])\n",
    "\n",
    "        # if type=2, test mode\n",
    "        else:\n",
    "            target_input = (ADC_data_list[ADC_train_length+ADC_valid_length:] \n",
    "                                + HGD_data_list[HGD_train_length + HGD_valid_length:] \n",
    "                                + LGD_data_list[LGD_train_length + LGD_valid_length:] \n",
    "                                + NOR_data_list[NOR_train_length + NOR_valid_length:])\n",
    "            target_label = (ADC_labeled_list[ADC_train_length + ADC_valid_length:] \n",
    "                                + HGD_labeled_list[HGD_train_length + HGD_valid_length:]\n",
    "                                + LGD_labeled_list[LGD_train_length + LGD_valid_length:] \n",
    "                                + NOR_labeled_list[NOR_train_length + NOR_valid_length:])\n",
    "\n",
    "        self.lst_label = target_label\n",
    "        self.lst_input = target_input\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        datapath = ''\n",
    "        if 'ADC' in self.lst_input[index]: # 0\n",
    "            datapath = os.path.join(self.data_dir, 'ADC')\n",
    "        elif 'HGD' in self.lst_input[index]: # 1\n",
    "            datapath = os.path.join(self.data_dir, 'HGD')\n",
    "        elif 'LGD' in self.lst_input[index]: # 2\n",
    "            datapath = os.path.join(self.data_dir, 'LGD')\n",
    "        elif 'NOR' in self.lst_input[index]: # 3\n",
    "            datapath = os.path.join(self.data_dir, 'NOR')\n",
    "        \n",
    "        input = Image.open(os.path.join(datapath, self.lst_input[index])).convert('RGB')\n",
    "        label = np.asarray(Image.open(os.path.join(datapath, self.lst_label[index])).convert('L').resize((256,256)))\n",
    "\n",
    "        # 정규화\n",
    "        label[label>150] = 255\n",
    "        label[label<=150] = 0\n",
    "        \n",
    "        label = label/255.0\n",
    "\n",
    "\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "\n",
    "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "\n",
    "        data = {'input': input, 'label': torch.from_numpy(label)}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4; test_batch_size = 1\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256,256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ]),\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((256,256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                          std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.590, 0.351, 0.259],\n",
    "                                 std=[0.241, 0.199, 0.163])\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': Dataset(data_dir=original_data_path, transform=data_transforms['train'], type=0),\n",
    "    'validation': Dataset(data_dir=original_data_path, transform=data_transforms['validation'], type=1),\n",
    "    'test': Dataset(data_dir=original_data_path, transform=data_transforms['test'], type=2) \n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=8),\n",
    "    'validation': DataLoader(image_datasets['validation'], batch_size=batch_size, shuffle=False, num_workers=8),\n",
    "    'test': DataLoader(image_datasets['test'], batch_size=test_batch_size, shuffle=False, num_workers=8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 설계\n",
    "\n",
    "- Resnet50을 backbone으로 하는 U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.with_nonlinearity = with_nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.with_nonlinearity:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Bridge(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bridge = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            ConvBlock(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bridge(x)\n",
    "\n",
    "class UpBlockForUNetWithResNet50(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
    "                 upsampling_method=\"conv_transpose\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if up_conv_in_channels == None:\n",
    "            up_conv_in_channels = in_channels\n",
    "        if up_conv_out_channels == None:\n",
    "            up_conv_out_channels = out_channels\n",
    "\n",
    "        if upsampling_method == \"conv_transpose\":\n",
    "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
    "        elif upsampling_method == \"bilinear\":\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            )\n",
    "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
    "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, up_x, down_x):\n",
    "        \"\"\"\n",
    "        :param up_x: this is the output from the previous up block\n",
    "        :param down_x: this is the output from the down block\n",
    "        :return: upsampled feature map\n",
    "        \"\"\"\n",
    "        x = self.upsample(up_x)\n",
    "        x = torch.cat([x, down_x], 1)\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        return x\n",
    "\n",
    "class UNetWithResnet50Encoder(nn.Module):\n",
    "    DEPTH = 6\n",
    "\n",
    "    def __init__(self, n_classes=4):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
    "        down_blocks = []\n",
    "        up_blocks = []\n",
    "        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
    "        self.input_pool = list(resnet.children())[3]\n",
    "        for bottleneck in list(resnet.children()):\n",
    "            if isinstance(bottleneck, nn.Sequential):\n",
    "                down_blocks.append(bottleneck)\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "        self.bridge = Bridge(2048, 2048)\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
    "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
    "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
    "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x, with_output_feature_map=False):\n",
    "        pre_pools = dict()\n",
    "        pre_pools[f\"layer_0\"] = x\n",
    "        x = self.input_block(x)\n",
    "        pre_pools[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
    "                continue\n",
    "            pre_pools[f\"layer_{i}\"] = x\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks, 1):\n",
    "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
    "            x = block(x, pre_pools[key])\n",
    "        output_feature_map = x\n",
    "        x = self.out(x)\n",
    "        del pre_pools\n",
    "        if with_output_feature_map:\n",
    "            return x, output_feature_map\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = UNetWithResnet50Encoder().to(device)\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 loss 함수 정의와 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "checkpoint_path = os.path.join(base_dir, 'chk')\n",
    "train_loss = []; val_loss = []\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "def train_model(model, optimizer, scheduler, st_epoch, num_epochs=25):\n",
    "    best_loss = 1e10\n",
    "    if len(os.listdir(os.path.join(base_dir, 'loss'))) != 0:\n",
    "        loss_info_path = os.path.join(base_dir, 'loss/data_dict.pkl')\n",
    "        with open(loss_info_path, 'rb') as f:\n",
    "            mydict = pickle.load(f)\n",
    "        mydict['val_loss'][-1] = best_loss\n",
    "\n",
    "    for epoch in range(st_epoch+1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs)); print('-' * 10)\n",
    "\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for batch, data in enumerate(dataloaders[phase]):\n",
    "                if batch % 100 == 0:\n",
    "                    print('{}/{}'.format(batch, len(dataloaders[phase])))\n",
    "                labels = data['label'].to(device)\n",
    "                inputs = data['input'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                tmp = F.softmax(outputs,1)\n",
    "                tmp = torch.sum(tmp[:, :2, :, :], dim=1)\n",
    "                tmp = tmp.unsqueeze(1)\n",
    "\n",
    "                loss = calc_loss(tmp, labels, metrics)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # zero the parameter gradients\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            \n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "            \n",
    "            # save the model weights\n",
    "            if phase == 'validation':\n",
    "                val_loss.append(epoch_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(f\"saving best model to {checkpoint_path}\")\n",
    "                    best_loss = epoch_loss\n",
    "                    save(ckpt_dir=checkpoint_path, model=model, optim=optimizer, epoch=epoch)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{} loss: {:.4f}'.format(phase, epoch_loss))\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    model, optim, st_epoch = load(ckpt_dir=checkpoint_path, model=model, optim=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "\n",
    "st_epoch = 0\n",
    "num_class = 4\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, st_epoch, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer_ft, st_epoch = load(ckpt_dir=checkpoint_path, model=model, optim=optimizer_ft)\n",
    "print(st_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 및 Accuracy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# loss 및 acc 정보 저장\n",
    "loss_dict = {}\n",
    "loss_dict['train_loss'] = train_loss; loss_dict['val_loss'] = val_loss\n",
    "\n",
    "# loss_dict 저장\n",
    "with open('data_dict.pkl','wb') as f:\n",
    "    pickle.dump(loss_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# loss_dict 불러오기 os.path.join(base_dir, 'loss/data_dict.pkl')\n",
    "loss_info_path = os.path.join(base_dir, 'loss/data_dict.pkl')\n",
    "with open(loss_info_path,'rb') as f:\n",
    "    mydict = pickle.load(f)\n",
    "\n",
    "train_loss = mydict['train_loss']\n",
    "val_loss = mydict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 시각화\n",
    "plt.figure(figsize=(15, 6)) \n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('loss'); plt.xlabel('epoch')\n",
    "plt.plot(train_loss, 'b', label='train loss')\n",
    "plt.plot(val_loss, 'g', label='val loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeTensor(tensor, labelmode=0, threshold=0.1):\n",
    "    image = tensor.detach().cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    if labelmode == 1:\n",
    "        image[image>threshold] = 1\n",
    "        image[image<=threshold] = 0 \n",
    "    \n",
    "    return image\n",
    "    \n",
    "# overlay images\n",
    "def do_overlay(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel Accuracy / Intersection Over Union / Dice coefficient(=F1 score)\n",
    "def get_PA_IOU_and_DICE(pred, true, TH):\n",
    "    pred = changeTensor(pred, 1, TH)\n",
    "    true = changeTensor(true, 1, TH)\n",
    "    \n",
    "    pred = pred.detach().cpu().numpy().reshape(-1)\n",
    "    true = true.detach().cpu().numpy().reshape(-1)\n",
    "    \n",
    "    size = len(pred)^2\n",
    "    pa = (size - np.logical_xor(true, pred).sum()) / size\n",
    "    \n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    \n",
    "    if np.sum(union) == 0:\n",
    "        iou = -1\n",
    "    else:\n",
    "        iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    intersection = (pred * true).sum()\n",
    "    dice = (2.*intersection + 1) / (pred.sum() + true.sum() + 1)\n",
    "\n",
    "    return pa, iou, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation 결과 평가\n",
    "TH = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "# Results(various metrics) for Training set\n",
    "#TH = [.1]\n",
    "print(len(dataloaders['test']))\n",
    "for th in TH:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc_PA = []\n",
    "        acc_IOU = []\n",
    "        acc_DICE = []\n",
    "        \n",
    "        for batch, data in enumerate(dataloaders['test'], 1):\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            output = model(input)\n",
    "\n",
    "            output = torch.sum(output[:, :2, :, :], dim=1)\n",
    "            \n",
    "            # # denormalize to show original image\n",
    "            # # mean=[0.590, 0.351, 0.259], std=[0.241, 0.199, 0.163]\n",
    "            # og = input.cpu().squeeze(0).permute(1,2,0).numpy()\n",
    "            # og[:,:,0] = ((og[:,:,0]) * 0.241) + 0.590\n",
    "            # og[:,:,1] = ((og[:,:,1]) * 0.199) + 0.351\n",
    "            # og[:,:,2] = ((og[:,:,2]) * 0.163) + 0.259\n",
    "\n",
    "            # # Calculate PA & IOU & Dice coefficient(=F1 score)\n",
    "            pa, iou, dice = get_PA_IOU_and_DICE(output, label, th)\n",
    "            acc_PA.append(pa)\n",
    "            acc_IOU.append(iou)\n",
    "            acc_DICE.append(dice)\n",
    "\n",
    "            # plt.figure(figsize=(8,8))\n",
    "            # # 불러온 이미지 시각화\n",
    "            # input_image_example = plt.subplot(1,3,1)\n",
    "            # input_image_example.set_title('input Image')\n",
    "            # plt.imshow(og)\n",
    "\n",
    "            # label_image_example = plt.subplot(1,3,2)\n",
    "            # label_image_example.set_title('Label Image')\n",
    "            # plt.imshow(to_pil_image(changeTensor(label)))\n",
    "\n",
    "            # test_image_example = plt.subplot(1,3,3)\n",
    "            # test_image_example.set_title('Prediction Image')\n",
    "            # plt.imshow(to_pil_image(changeTensor(output, 1)))\n",
    "\n",
    "            # plt.show()\n",
    "            # plt.close('all')\n",
    "            # plt.clf()\n",
    "\n",
    "        del acc_IOU[202:]\n",
    "        del acc_DICE[202:]\n",
    "        print(np.round(np.mean(acc_PA),4))\n",
    "        print(np.round(np.mean(acc_IOU),4))\n",
    "        print(np.round(np.mean(acc_DICE), 4))\n",
    "        print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation map 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Activation map 결과 평가\n",
    "TH = [.1]\n",
    "for th in TH:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch, data in enumerate(dataloaders['test'], 1):\n",
    "            if batch < 10:\n",
    "                continue\n",
    "            if batch == 15:\n",
    "                break\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            output = model(input)\n",
    "            print(output.shape)\n",
    "            output = output.cpu().squeeze(0).permute(1,2,0).numpy()\n",
    "            tmp = np.dsplit(output, 4)\n",
    "            result = cv2.addWeighted(tmp[0], 0.5, tmp[1], 0.5, 0)\n",
    "            result = cv2.addWeighted(result, 0.5, tmp[2], 0.5, 0)\n",
    "            result = cv2.addWeighted(result, 0.5, tmp[3], 0.5, 0)\n",
    "            \n",
    "            # denormalize to show original image\n",
    "            # mean=[0.590, 0.351, 0.259], std=[0.241, 0.199, 0.163]\n",
    "            og = input.cpu().squeeze(0).permute(1,2,0).numpy()\n",
    "            og[:,:,0] = ((og[:,:,0]) * 0.241) + 0.590\n",
    "            og[:,:,1] = ((og[:,:,1]) * 0.199) + 0.351\n",
    "            og[:,:,2] = ((og[:,:,2]) * 0.163) + 0.259\n",
    "\n",
    "            plt.figure(figsize=(8,8))\n",
    "            # 불러온 이미지 시각화\n",
    "            input_image_example = plt.subplot(1,3,1)\n",
    "            input_image_example.set_title('input Image')\n",
    "            plt.imshow(og)\n",
    "\n",
    "            label_image_example = plt.subplot(1,3,2)\n",
    "            label_image_example.set_title('Label Image')\n",
    "            plt.imshow(to_pil_image(changeTensor(label)))\n",
    "\n",
    "            test_image_example = plt.subplot(1,3,3)\n",
    "            test_image_example.set_title('Prediction Image')\n",
    "            # plt.imshow(result)\n",
    "            plt.imshow(output[:, :, 0], 'Reds_r')\n",
    "            # plt.imshow(output[:, :, 1], 'Reds_r')\n",
    "            # plt.imshow(output[:, :, 2], 'Reds_r')\n",
    "            # plt.imshow(output[:, :, 3], 'Reds_r')\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 파라미터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = Image.open(os.path.join('./', 'Prediction_' + str(batch) + '.png'))\n",
    "            #plt.imshow(r)\n",
    "\n",
    "            # plt.imshow(to_pil_image(changeTensor(output, 1)))\n",
    "\n",
    "            # change_example = plt.subplot(1,4,4)\n",
    "            # change_example.set_title('Change Image')\n",
    "            # r = Image.open(os.path.join('./', 'Prediction_' + str(batch) + '.png'))\n",
    "            # plt.imshow(r)\n",
    "\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            plt.clf()\n",
    "            \n",
    "            # # 이미지 저장\n",
    "            # img = changeTensor(output,1).cpu().numpy()\n",
    "            # img = np.swapaxes(img, 0, 1)\n",
    "            # img = np.swapaxes(img, 1, 2)\n",
    "            \n",
    "            # for x in img:\n",
    "            #     for y in x:\n",
    "            #         # 흰색은 빨간색으로\n",
    "            #         if y[0] == 1 and y[1] == 1 and y[2] == 1:\n",
    "            #             y[0] = 1\n",
    "            #             y[1] = y[2] = 0\n",
    "\n",
    "            #         elif y[0] == 1 and y[1] == 0 and y[2] == 0:\n",
    "            #             y[0] = y[2] = 0\n",
    "            #             y[1] = 1\n",
    "            \n",
    "            # plt.imsave('Prediction_' + str(batch) +'.png', img)\n",
    "\n",
    "        del acc_IOU[202:]\n",
    "        del acc_DICE[202:]\n",
    "        print(np.round(np.mean(acc_PA),4))\n",
    "        print(np.round(np.mean(acc_IOU),4))\n",
    "        print(np.round(np.mean(acc_DICE), 4))\n",
    "        print(\"-\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a28fd041cca8e88761e305ad12b9793255fd9d220f898ef66faf7da0cb355b51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
